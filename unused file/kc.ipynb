{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG96T7J9dQyc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ7-uJzu34j7",
        "outputId": "6c3ef8fe-6d7f-4615-ff1d-52ba40c2592c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-g_o2kge7/kobert-tokenizer_52421acdc25e4e4390923313b0b7a9cb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-g_o2kge7/kobert-tokenizer_52421acdc25e4e4390923313b0b7a9cb\n",
            "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kobert_tokenizer\n",
            "  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4632 sha256=e085f3b2204a1c2b2a437c61149cce5135b5ae0166d782a04d778a144f8f6a35\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3qjy2x2w/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n",
            "Successfully built kobert_tokenizer\n",
            "Installing collected packages: kobert_tokenizer\n",
            "Successfully installed kobert_tokenizer-0.1\n"
          ]
        }
      ],
      "source": [
        "'''!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3vBmwdz7amE",
        "outputId": "3298ea83-fad9-487e-d82f-7f641e81bdf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.10/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.31.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2023.11.17)\n",
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (1.23.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (3.0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!git clone https://github.com/e9t/nsmc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxogfIQy4Wcd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0hkFFsuw_tk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import BertModel\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aquUgda7zB3-"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-large\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"beomi/kcbert-large\")\n",
        "\n",
        "#vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMR9VkSyzR6Y"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOErwE8sueTc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('mbti_100_4_ns.csv',encoding='UTF-8')\n",
        "df = df.drop('Unnamed: 0',axis = 1)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciLXVZAg1519"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE6N5zna1Dcd"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(axis=0)\n",
        "\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf7NJCzUu6XN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "df.loc[(df['mbti'] == \"intj\"), 'mbti'] = 0\n",
        "df.loc[(df['mbti'] == \"intp\"), 'mbti'] = 1\n",
        "df.loc[(df['mbti'] == \"entj\"), 'mbti'] = 2\n",
        "df.loc[(df['mbti'] == \"entp\"), 'mbti'] = 3\n",
        "df.loc[(df['mbti'] == \"infj\"), 'mbti'] = 4\n",
        "df.loc[(df['mbti'] == \"infp\"), 'mbti'] = 5\n",
        "df.loc[(df['mbti'] == \"enfj\"), 'mbti'] = 6\n",
        "df.loc[(df['mbti'] == \"enfp\"), 'mbti'] = 7\n",
        "df.loc[(df['mbti'] == \"istj\"), 'mbti'] = 8\n",
        "df.loc[(df['mbti'] == \"istp\"), 'mbti'] = 9\n",
        "df.loc[(df['mbti'] == \"estj\"), 'mbti'] = 10\n",
        "df.loc[(df['mbti'] == \"estp\"), 'mbti'] = 11\n",
        "df.loc[(df['mbti'] == \"isfj\"), 'mbti'] = 12\n",
        "df.loc[(df['mbti'] == \"isfp\"), 'mbti'] = 13\n",
        "df.loc[(df['mbti'] == \"esfj\"), 'mbti'] = 14\n",
        "df.loc[(df['mbti'] == \"esfp\"), 'mbti'] = 15\n",
        "'''\n",
        "df.loc[(df['ns'] == \"n\"), 'ns'] = 0\n",
        "df.loc[(df['ns'] == \"s\"), 'ns'] = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmSuApvk5sRs"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnMbPPrB2wqA"
      },
      "outputs": [],
      "source": [
        "data_list = []\n",
        "for q, label in zip(df['text'], df['ns'])  :\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "\n",
        "    data_list.append(data)\n",
        "\n",
        "data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KagF8ZzZ5-Tc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test = train_test_split(df, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers pytorch_lightning emoji soynlp"
      ],
      "metadata": {
        "id": "B7X_adrlSXNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import re\n",
        "import emoji\n",
        "from soynlp.normalizer import repeat_normalize"
      ],
      "metadata": {
        "id": "EK9R1vxNSTAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "188EdFuJw1Wf"
      },
      "outputs": [],
      "source": [
        "class Arg:\n",
        "    random_seed: int = 42  # Random Seed\n",
        "    pretrained_model: str = 'beomi/kcbert-large'  # Transformers PLM name\n",
        "    pretrained_tokenizer: str = ''  # Optional, Transformers Tokenizer Name. Overrides `pretrained_model`\n",
        "    auto_batch_size: str = 'power'  # Let PyTorch Lightening find the best batch size\n",
        "    batch_size: int = 0  # Optional, Train/Eval Batch Size. Overrides `auto_batch_size`\n",
        "    lr: float = 5e-6  # Starting Learning Rate\n",
        "    epochs: int = 20  # Max Epochs\n",
        "    max_length: int = 150  # Max Length input size\n",
        "    report_cycle: int = 100  # Report (Train Metrics) Cycle\n",
        "    train_data: list =  X_train # Train Dataset file\n",
        "    val_data: list = X_test  # Validation Dataset file\n",
        "    cpu_workers: int = os.cpu_count()  # Multi cpu workers\n",
        "    test_mode: bool = False  # Test Mode enables `fast_dev_run`\n",
        "    optimizer: str = 'AdamW'  # AdamW vs AdamP\n",
        "    lr_scheduler: str = 'exp'  # ExponentialLR vs CosineAnnealingWarmRestarts\n",
        "    fp16: bool = False  # Enable train on FP16\n",
        "    tpu_cores: int = 0  # Enable TPU with 1 core or 8 cores\n",
        "\n",
        "args = Arg()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args.train_data\n",
        "a = torch.tensor(args.train_data['text'].to_list(), dtype=torch.long)\n",
        "a"
      ],
      "metadata": {
        "id": "IGLBzPUfgN65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "t_HbX3z0eZhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(LightningModule):\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        self.arg = options\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(self.arg.pretrained_model)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\n",
        "            self.arg.pretrained_tokenizer\n",
        "            if self.arg.pretrained_tokenizer\n",
        "            else self.arg.pretrained_model\n",
        "        )\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        return self.bert(**kwargs)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        data, labels = batch\n",
        "        output = self(input_ids=data, labels=labels)\n",
        "\n",
        "        # Transformers 4.0.0+\n",
        "        loss = output.loss\n",
        "        logits = output.logits\n",
        "\n",
        "        preds = logits.argmax(dim=-1)\n",
        "\n",
        "        y_true = labels.cpu().numpy()\n",
        "        y_pred = preds.cpu().numpy()\n",
        "\n",
        "        # Acc, Precision, Recall, F1\n",
        "        metrics = [\n",
        "            metric(y_true=y_true, y_pred=y_pred)\n",
        "            for metric in\n",
        "            (accuracy_score, precision_score, recall_score, f1_score)\n",
        "        ]\n",
        "\n",
        "        tensorboard_logs = {\n",
        "            'train_loss': loss.cpu().detach().numpy().tolist(),\n",
        "            'train_acc': metrics[0],\n",
        "            'train_precision': metrics[1],\n",
        "            'train_recall': metrics[2],\n",
        "            'train_f1': metrics[3],\n",
        "        }\n",
        "        if (batch_idx % self.arg.report_cycle) == 0:\n",
        "            print()\n",
        "            pprint(tensorboard_logs)\n",
        "        return {'loss': loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        data, labels = batch\n",
        "        output = self(input_ids=data, labels=labels)\n",
        "\n",
        "        # Transformers 4.0.0+\n",
        "        loss = output.loss\n",
        "        logits = output.logits\n",
        "\n",
        "        preds = logits.argmax(dim=-1)\n",
        "\n",
        "        y_true = list(labels.cpu().numpy())\n",
        "        y_pred = list(preds.cpu().numpy())\n",
        "\n",
        "        return {\n",
        "            'loss': loss,\n",
        "            'y_true': y_true,\n",
        "            'y_pred': y_pred,\n",
        "        }\n",
        "\n",
        "    def on_validation_epoch_end(self, outputs):\n",
        "        loss = torch.tensor(0, dtype=torch.float)\n",
        "        for i in outputs:\n",
        "            loss += i['loss'].cpu().detach()\n",
        "        _loss = loss / len(outputs)\n",
        "\n",
        "        loss = float(_loss)\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "\n",
        "        for i in outputs:\n",
        "            y_true += i['y_true']\n",
        "            y_pred += i['y_pred']\n",
        "\n",
        "        # Acc, Precision, Recall, F1\n",
        "        metrics = [\n",
        "            metric(y_true=y_true, y_pred=y_pred)\n",
        "            for metric in\n",
        "            (accuracy_score, precision_score, recall_score, f1_score)\n",
        "        ]\n",
        "\n",
        "        tensorboard_logs = {\n",
        "            'val_loss': loss,\n",
        "            'val_acc': metrics[0],\n",
        "            'val_precision': metrics[1],\n",
        "            'val_recall': metrics[2],\n",
        "            'val_f1': metrics[3],\n",
        "        }\n",
        "\n",
        "        print()\n",
        "        pprint(tensorboard_logs)\n",
        "        return {'loss': _loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        if self.arg.optimizer == 'AdamW':\n",
        "            optimizer = AdamW(self.parameters(), lr=self.arg.lr)\n",
        "        elif self.arg.optimizer == 'AdamP':\n",
        "            from adamp import AdamP\n",
        "            optimizer = AdamP(self.parameters(), lr=self.arg.lr)\n",
        "        else:\n",
        "            raise NotImplementedError('Only AdamW and AdamP is Supported!')\n",
        "        if self.arg.lr_scheduler == 'cos':\n",
        "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
        "        elif self.arg.lr_scheduler == 'exp':\n",
        "            scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
        "        else:\n",
        "            raise NotImplementedError('Only cos and exp lr scheduler is Supported!')\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'scheduler': scheduler,\n",
        "        }\n",
        "\n",
        "    def read_data(self, path):\n",
        "        if path.endswith('xlsx'):\n",
        "            return pd.read_excel(path)\n",
        "        elif path.endswith('csv'):\n",
        "            return pd.read_csv(path)\n",
        "        elif path.endswith('tsv') or path.endswith('txt'):\n",
        "            return pd.read_csv(path, sep='\\t')\n",
        "        else:\n",
        "            raise NotImplementedError('Only Excel(xlsx)/Csv/Tsv(txt) are Supported')\n",
        "\n",
        "    def preprocess_dataframe(self, df):\n",
        "        pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣]+')\n",
        "        url_pattern = re.compile(\n",
        "            r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
        "\n",
        "        def clean(x):\n",
        "            x = pattern.sub(' ', x)\n",
        "            x = url_pattern.sub('', x)\n",
        "            x = x.strip()\n",
        "            x = repeat_normalize(x, num_repeats=2)\n",
        "            return x\n",
        "\n",
        "        df['text'] = df['text'].map(lambda x: self.tokenizer.encode(\n",
        "            clean(str(x)),\n",
        "            padding='max_length',\n",
        "            max_length=self.arg.max_length,\n",
        "            truncation=True,\n",
        "        ))\n",
        "        return df\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        df = self.arg.train_data\n",
        "        df = self.preprocess_dataframe(df)\n",
        "\n",
        "\n",
        "        dataset = TensorDataset(\n",
        "            torch.tensor(df['text'].to_list(), dtype=torch.long),\n",
        "            torch.tensor(df['ns'].to_list(), dtype=torch.long),\n",
        "        )\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.arg.batch_size or self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.arg.cpu_workers,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        df = self.arg.val_data\n",
        "        df = self.preprocess_dataframe(df)\n",
        "\n",
        "        dataset = TensorDataset(\n",
        "            torch.tensor(df['text'].to_list(), dtype=torch.long),\n",
        "            torch.tensor(df['ns'].to_list(), dtype=torch.long),\n",
        "        )\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.arg.batch_size or self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.arg.cpu_workers,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "RtDMhjDsS2QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# args.tpu_cores = 8  # Enables TPU\n",
        "args.fp16 = True  # Enables GPU FP16\n",
        "args.batch_size = 16  # Force setup batch_size"
      ],
      "metadata": {
        "id": "SHNLdZLVZAyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "117pAxLlB6Zn"
      },
      "outputs": [],
      "source": [
        "print(\"Using PyTorch Ver\", torch.__version__)\n",
        "print(\"Fix Seed:\", args.random_seed)\n",
        "seed_everything(args.random_seed)\n",
        "model = Model(args)\n",
        "\n",
        "print(\":: Start Training ::\")\n",
        "\n",
        "trainer = Trainer(\n",
        "        epochs=args.epochs,\n",
        "        fast_dev_run=args.test_mode,\n",
        "        num_sanity_val_steps=None if args.test_mode else 0,\n",
        "        # For GPU Setup\n",
        "        deterministic=torch.cuda.is_available(),\n",
        "        precision=16 if args.fp16 else 32,\n",
        "        # For TPU Setup\n",
        "        # tpu_cores=args.tpu_cores if args.tpu_cores else None,\n",
        "    )\n",
        "trainer.fit(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "94ixvWj-VdFb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}